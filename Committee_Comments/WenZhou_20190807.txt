I do not have too much comments in details. But just suggest you can consider

1) the ROC curve as an option instead of the \chi^2 statistics to demonstrate the difference between POD and other method (if you did make ROC curve, the difference between two curves can be statistically quantified) and then you will be able to say on which part of the parameter space, your method is better than others and vice versa. This will suggest the limitation of your method and also provide guidance for future researchers if they would like to follow. This could be in the discussion at least if you do not have time to make one but just point it out.
2) Different penalty: the penalty you used is so-called Ridge or weighted Ridge regression (the V matrix serves to define the Mahalobis distance), in general there should be a tuning parameter lambda in front of your penalty term (this is not just for tuning but it has mathematical reason to balance the competition of the objective function, your likelihood, and the penalty) and theoretically, that tuning parameter has an optimal scale. For example, if the data size is M and parameter is 560, then that tuning parameter should be of the order \sqrt{s\log(560)/M} where s is the number of parameters one believes matters (that is nonzero parameter). This is quite theoretical, but if you use this math then such a well-known results should not be ignored. Best reference for this (for your purpose) is the classical book by Tibshirani, Hastie and Friedman, Statistical Machine Learning. This type of penalty is called regularization in mathematics or statistics; book by Stephen Byod (convex optimization, the bible book for convex optimization) is all a good reference.
3) L_1 penalty. Since you have ~500 parameters, it is more reasonable to believe that many of them are not related to the model or process. Then, instead of multivariate normal (in Bayesian language, you put a prior to the parameter), you can use ||y||_1 =\sum_{k=1}^{560}|y_k| as the penalty. In the world of Bayesian, this is Laplace prior distribution if you have to go with MCMC. This penalty will promote exactly 0 value to non-important parameter, and is known as LASSO (it is one of most important work in last century for statistics, mathematics, information theory and others). For Poisson likelihood, the method is sometimes known as glm-Lasso or PAM (both invented and coded in package for Tibshirani et al in Stanford). You can, for example, mention this in the discuss section.

The penalty modeling is more data-driven and need more thinking in fact. You do not have to change the thesis, but you can mention other possibilities: for example, ||y||_2+||y||_1, which is the famous elastic net (it will top your model most likely given my experience), or y'V^{-1}y+||y||_1 (the V part is known as graphical Laplacian to capture how parameters should be connected), or ||y_ext||_2+||y_flux||_2+||y_??||_2... this is known as group lasso, etc. I appreciate your effort on penalized or regularize the likelihood using Ridge penalty (y'V^{-1}y) but other choices should be considered or mentioned. The best reference for all of them is the book by Martin Wainwright, Tibshirani, Hastie, Sparse Learning.

If they take too much of your time, you can discuss about them in the discussion at least?
